<!DOCTYPE html><html lang="en" class="scroll-smooth" style="scroll-padding-top:100px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/projects/spatial-robotics/thumbnail.png" fetchPriority="high"/><link rel="stylesheet" href="/_next/static/css/6bf93e6ed8c34575.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b2135b9b623eb486.js"/><script src="/_next/static/chunks/fd9d1056-91aa9495991b9c80.js" async=""></script><script src="/_next/static/chunks/117-1456bf8bcd1cfcce.js" async=""></script><script src="/_next/static/chunks/main-app-c3db31d29f1b7be3.js" async=""></script><script src="/_next/static/chunks/24-64d1f4b8e8124fbe.js" async=""></script><script src="/_next/static/chunks/app/project/spatial-robotics/page-87f9f285f456cb87.js" async=""></script><title>Teng Teng - Lead of AI/ML Development &amp; Research</title><meta name="description" content="Personal portfolio of Teng Teng, Lead of AI/ML Development &amp; Research at SAP"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><link rel="icon" href="/favicon.ico"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans"><div class="min-h-screen bg-gray-50"><header class="bg-white shadow-sm fixed top-0 left-0 right-0 z-50"><div class="container mx-auto px-4 py-4"><a class="text-blue-600 hover:text-blue-800 flex items-center" href="/#featured"><svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 19l-7-7m0 0l7-7m-7 7h18"></path></svg>Back to Home</a></div></header><main class="container mx-auto px-4 pt-24 pb-12"><div class="max-w-4xl mx-auto" style="opacity:0;will-change:opacity,transform;transform:translateY(20px)"><div class="bg-white rounded-lg shadow-lg overflow-hidden"><div class="relative h-96"><img alt="Spatial-AI in Collaborative Robotics" fetchPriority="high" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/projects/spatial-robotics/thumbnail.png"/></div><div class="p-8 space-y-8"><div><h1 class="text-3xl font-bold mb-4">Spatial-AI in Collaborative Robotics</h1><div class="flex flex-wrap gap-2 mb-4"><span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">ROS</span><span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">Python</span><span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">C++</span><span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">RealSense SDK</span><span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">Point Cloud Library</span><span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">OpenCV</span><span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">TF (Transform)</span><span class="px-3 py-1 bg-blue-100 text-blue-800 rounded-full text-sm">myCobot API</span></div><p class="text-gray-600">Integration of RealSense D455 depth camera with myCobot robotic arm for spatial awareness and object manipulation using ROS.</p></div><div><h2 class="text-xl font-semibold mb-2">Challenge</h2><p class="text-gray-600 whitespace-pre-line">Integrating spatial awareness into robotic systems presents significant challenges in coordinate transformation, real-time processing, and precise control. Traditional robotic systems often lack the ability to accurately perceive and interact with their environment, making human-robot collaboration difficult and potentially unsafe. The challenge was to develop a system that could seamlessly integrate depth sensing with robotic control.</p></div><div><h2 class="text-xl font-semibold mb-2">Solution</h2><p class="text-gray-600 whitespace-pre-line">As the technical lead, I developed a comprehensive ROS-based system that integrates Intel RealSense D455 depth camera with myCobot robotic arm. The solution includes custom calibration methods for coordinate transformation, real-time point cloud processing for object detection, and precise control algorithms for robotic manipulation. The system uses ROS nodes for communication between the depth camera and robot controller.</p><div class="my-8"></div></div><div><h2 class="text-xl font-semibold mb-2">Impact</h2><p class="text-gray-600 whitespace-pre-line">The implementation has enabled precise object tracking and manipulation capabilities. The system successfully demonstrates real-time spatial awareness, allowing the robot to accurately locate and interact with objects in its environment. The integration provides a foundation for advanced human-robot collaboration applications.</p></div><section class="space-y-8"><div><h2 class="text-xl font-semibold mb-4">System Architecture</h2><div class="space-y-6"><div class="relative h-[600px] w-full rounded-lg overflow-hidden"><img alt="System Architecture" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/projects/spatial-robotics/architecture.png"/></div><div class="space-y-6"><div><h3 class="font-medium text-lg mb-3">Hardware Integration</h3><p class="text-gray-600">The system integrates a myCobot 280 M5Stack robotic arm with an Intel RealSense D455 depth camera. The D455 provides high-quality depth sensing and RGB imaging, while the 6-DOF robotic arm enables precise manipulation. The hardware is coordinated through ROS nodes running on Ubuntu 20.04.</p><div class="relative h-[400px] w-full rounded-lg overflow-hidden mt-4"><img alt="myCobot Hardware Setup" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/projects/spatial-robotics/mycobot.png"/></div></div><div><h3 class="font-medium text-lg mb-3">Spatial Calibration</h3><p class="text-gray-600">Developed a custom calibration system using marker-based detection to establish the coordinate transformation between the camera and robot base frames. The calibration process uses three reference points to compute the translation and rotation matrices, enabling accurate coordinate mapping between vision and robot control systems.</p></div><div><h3 class="font-medium text-lg mb-3">Point Cloud Processing</h3><p class="text-gray-600">Implemented real-time point cloud processing pipelines for object detection and tracking. The system uses color-based segmentation and clustering algorithms to identify target objects, with custom ROS nodes handling the point cloud data processing and coordinate transformation for robot control.</p></div></div></div></div><div><h2 class="text-xl font-semibold mb-4">Implementation Details</h2><div class="space-y-4"><div class="bg-white p-6 rounded-lg shadow-sm"><h3 class="font-medium text-lg mb-2">ROS Framework</h3><p class="text-gray-600">The system utilizes ROS Noetic for communication between components. Custom nodes handle point cloud processing, coordinate transformation broadcasting, and robot control. The architecture includes nodes for color detection, position calculation, and robot movement coordination.</p></div><div class="bg-white p-6 rounded-lg shadow-sm"><h3 class="font-medium text-lg mb-2">Control System</h3><p class="text-gray-600">Developed a hybrid control system combining ROS MoveIt for path planning and direct control through the myCobot Python API. The system includes safety checks and real-time position monitoring to ensure reliable operation.</p></div></div></div></section><div class="flex gap-4 mt-8"></div></div></div></div></main></div><script src="/_next/static/chunks/webpack-b2135b9b623eb486.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/6bf93e6ed8c34575.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"2:I[2846,[],\"\"]\n4:I[1670,[\"24\",\"static/chunks/24-64d1f4b8e8124fbe.js\",\"463\",\"static/chunks/app/project/spatial-robotics/page-87f9f285f456cb87.js\"],\"default\"]\n5:I[5878,[\"24\",\"static/chunks/24-64d1f4b8e8124fbe.js\",\"463\",\"static/chunks/app/project/spatial-robotics/page-87f9f285f456cb87.js\"],\"Image\"]\n6:I[4707,[],\"\"]\n7:I[6423,[],\"\"]\n9:I[1060,[],\"\"]\na:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L2\",null,{\"buildId\":\"dvzg704mT_g1JwigrlbdN\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"project\",\"spatial-robotics\"],\"initialTree\":[\"\",{\"children\":[\"project\",{\"children\":[\"spatial-robotics\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"project\",{\"children\":[\"spatial-robotics\",{\"children\":[\"__PAGE__\",{},[[\"$L3\",[\"$\",\"$L4\",null,{\"title\":\"Spatial-AI in Collaborative Robotics\",\"description\":\"Integration of RealSense D455 depth camera with myCobot robotic arm for spatial awareness and object manipulation using ROS.\",\"challenge\":\"Integrating spatial awareness into robotic systems presents significant challenges in coordinate transformation, real-time processing, and precise control. Traditional robotic systems often lack the ability to accurately perceive and interact with their environment, making human-robot collaboration difficult and potentially unsafe. The challenge was to develop a system that could seamlessly integrate depth sensing with robotic control.\",\"solution\":\"As the technical lead, I developed a comprehensive ROS-based system that integrates Intel RealSense D455 depth camera with myCobot robotic arm. The solution includes custom calibration methods for coordinate transformation, real-time point cloud processing for object detection, and precise control algorithms for robotic manipulation. The system uses ROS nodes for communication between the depth camera and robot controller.\",\"impact\":\"The implementation has enabled precise object tracking and manipulation capabilities. The system successfully demonstrates real-time spatial awareness, allowing the robot to accurately locate and interact with objects in its environment. The integration provides a foundation for advanced human-robot collaboration applications.\",\"techStack\":[\"ROS\",\"Python\",\"C++\",\"RealSense SDK\",\"Point Cloud Library\",\"OpenCV\",\"TF (Transform)\",\"myCobot API\"],\"imageUrl\":\"/projects/spatial-robotics/thumbnail.png\",\"children\":[\"$\",\"section\",null,{\"className\":\"space-y-8\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-4\",\"children\":\"System Architecture\"}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative h-[600px] w-full rounded-lg overflow-hidden\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/projects/spatial-robotics/architecture.png\",\"alt\":\"System Architecture\",\"fill\":true,\"className\":\"object-contain\"}]}],[\"$\",\"div\",null,{\"className\":\"space-y-6\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"font-medium text-lg mb-3\",\"children\":\"Hardware Integration\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600\",\"children\":\"The system integrates a myCobot 280 M5Stack robotic arm with an Intel RealSense D455 depth camera. The D455 provides high-quality depth sensing and RGB imaging, while the 6-DOF robotic arm enables precise manipulation. The hardware is coordinated through ROS nodes running on Ubuntu 20.04.\"}],[\"$\",\"div\",null,{\"className\":\"relative h-[400px] w-full rounded-lg overflow-hidden mt-4\",\"children\":[\"$\",\"$L5\",null,{\"src\":\"/projects/spatial-robotics/mycobot.png\",\"alt\":\"myCobot Hardware Setup\",\"fill\":true,\"className\":\"object-contain\"}]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"font-medium text-lg mb-3\",\"children\":\"Spatial Calibration\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600\",\"children\":\"Developed a custom calibration system using marker-based detection to establish the coordinate transformation between the camera and robot base frames. The calibration process uses three reference points to compute the translation and rotation matrices, enabling accurate coordinate mapping between vision and robot control systems.\"}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"font-medium text-lg mb-3\",\"children\":\"Point Cloud Processing\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600\",\"children\":\"Implemented real-time point cloud processing pipelines for object detection and tracking. The system uses color-based segmentation and clustering algorithms to identify target objects, with custom ROS nodes handling the point cloud data processing and coordinate transformation for robot control.\"}]]}]]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-4\",\"children\":\"Implementation Details\"}],[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-white p-6 rounded-lg shadow-sm\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"font-medium text-lg mb-2\",\"children\":\"ROS Framework\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600\",\"children\":\"The system utilizes ROS Noetic for communication between components. Custom nodes handle point cloud processing, coordinate transformation broadcasting, and robot control. The architecture includes nodes for color detection, position calculation, and robot movement coordination.\"}]]}],[\"$\",\"div\",null,{\"className\":\"bg-white p-6 rounded-lg shadow-sm\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"font-medium text-lg mb-2\",\"children\":\"Control System\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600\",\"children\":\"Developed a hybrid control system combining ROS MoveIt for path planning and direct control through the myCobot Python API. The system includes safety checks and real-time position monitoring to ensure reliable operation.\"}]]}]]}]]}]]}]}],null],null],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"project\",\"children\",\"spatial-robotics\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"project\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/6bf93e6ed8c34575.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"style\":{\"scrollPaddingTop\":\"100px\"},\"children\":[\"$\",\"body\",null,{\"className\":\"font-sans\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$L8\"],\"globalErrorComponent\":\"$9\",\"missingSlots\":\"$Wa\"}]\n"])</script><script>self.__next_f.push([1,"8:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Teng Teng - Lead of AI/ML Development \u0026 Research\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Personal portfolio of Teng Teng, Lead of AI/ML Development \u0026 Research at SAP\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"5\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"meta\",\"6\",{\"name\":\"next-size-adjust\"}]]\n3:null\n"])</script></body></html>