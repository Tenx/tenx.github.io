2:I[1670,["24","static/chunks/24-64d1f4b8e8124fbe.js","463","static/chunks/app/project/spatial-robotics/page-87f9f285f456cb87.js"],"default"]
3:I[5878,["24","static/chunks/24-64d1f4b8e8124fbe.js","463","static/chunks/app/project/spatial-robotics/page-87f9f285f456cb87.js"],"Image"]
4:I[4707,[],""]
5:I[6423,[],""]
0:["dvzg704mT_g1JwigrlbdN",[[["",{"children":["project",{"children":["spatial-robotics",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",{"children":["project",{"children":["spatial-robotics",{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"title":"Spatial-AI in Collaborative Robotics","description":"Integration of RealSense D455 depth camera with myCobot robotic arm for spatial awareness and object manipulation using ROS.","challenge":"Integrating spatial awareness into robotic systems presents significant challenges in coordinate transformation, real-time processing, and precise control. Traditional robotic systems often lack the ability to accurately perceive and interact with their environment, making human-robot collaboration difficult and potentially unsafe. The challenge was to develop a system that could seamlessly integrate depth sensing with robotic control.","solution":"As the technical lead, I developed a comprehensive ROS-based system that integrates Intel RealSense D455 depth camera with myCobot robotic arm. The solution includes custom calibration methods for coordinate transformation, real-time point cloud processing for object detection, and precise control algorithms for robotic manipulation. The system uses ROS nodes for communication between the depth camera and robot controller.","impact":"The implementation has enabled precise object tracking and manipulation capabilities. The system successfully demonstrates real-time spatial awareness, allowing the robot to accurately locate and interact with objects in its environment. The integration provides a foundation for advanced human-robot collaboration applications.","techStack":["ROS","Python","C++","RealSense SDK","Point Cloud Library","OpenCV","TF (Transform)","myCobot API"],"imageUrl":"/projects/spatial-robotics/thumbnail.png","children":["$","section",null,{"className":"space-y-8","children":[["$","div",null,{"children":[["$","h2",null,{"className":"text-xl font-semibold mb-4","children":"System Architecture"}],["$","div",null,{"className":"space-y-6","children":[["$","div",null,{"className":"relative h-[600px] w-full rounded-lg overflow-hidden","children":["$","$L3",null,{"src":"/projects/spatial-robotics/architecture.png","alt":"System Architecture","fill":true,"className":"object-contain"}]}],["$","div",null,{"className":"space-y-6","children":[["$","div",null,{"children":[["$","h3",null,{"className":"font-medium text-lg mb-3","children":"Hardware Integration"}],["$","p",null,{"className":"text-gray-600","children":"The system integrates a myCobot 280 M5Stack robotic arm with an Intel RealSense D455 depth camera. The D455 provides high-quality depth sensing and RGB imaging, while the 6-DOF robotic arm enables precise manipulation. The hardware is coordinated through ROS nodes running on Ubuntu 20.04."}],["$","div",null,{"className":"relative h-[400px] w-full rounded-lg overflow-hidden mt-4","children":["$","$L3",null,{"src":"/projects/spatial-robotics/mycobot.png","alt":"myCobot Hardware Setup","fill":true,"className":"object-contain"}]}]]}],["$","div",null,{"children":[["$","h3",null,{"className":"font-medium text-lg mb-3","children":"Spatial Calibration"}],["$","p",null,{"className":"text-gray-600","children":"Developed a custom calibration system using marker-based detection to establish the coordinate transformation between the camera and robot base frames. The calibration process uses three reference points to compute the translation and rotation matrices, enabling accurate coordinate mapping between vision and robot control systems."}]]}],["$","div",null,{"children":[["$","h3",null,{"className":"font-medium text-lg mb-3","children":"Point Cloud Processing"}],["$","p",null,{"className":"text-gray-600","children":"Implemented real-time point cloud processing pipelines for object detection and tracking. The system uses color-based segmentation and clustering algorithms to identify target objects, with custom ROS nodes handling the point cloud data processing and coordinate transformation for robot control."}]]}]]}]]}]]}],["$","div",null,{"children":[["$","h2",null,{"className":"text-xl font-semibold mb-4","children":"Implementation Details"}],["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"bg-white p-6 rounded-lg shadow-sm","children":[["$","h3",null,{"className":"font-medium text-lg mb-2","children":"ROS Framework"}],["$","p",null,{"className":"text-gray-600","children":"The system utilizes ROS Noetic for communication between components. Custom nodes handle point cloud processing, coordinate transformation broadcasting, and robot control. The architecture includes nodes for color detection, position calculation, and robot movement coordination."}]]}],["$","div",null,{"className":"bg-white p-6 rounded-lg shadow-sm","children":[["$","h3",null,{"className":"font-medium text-lg mb-2","children":"Control System"}],["$","p",null,{"className":"text-gray-600","children":"Developed a hybrid control system combining ROS MoveIt for path planning and direct control through the myCobot Python API. The system includes safety checks and real-time position monitoring to ensure reliable operation."}]]}]]}]]}]]}]}],null],null],null]},[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","project","children","spatial-robotics","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children","project","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/6bf93e6ed8c34575.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","style":{"scrollPaddingTop":"100px"},"children":["$","body",null,{"className":"font-sans","children":["$","$L4",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}]}]],null],null],["$L6",null]]]]
6:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Teng Teng - Lead of AI/ML Development & Research"}],["$","meta","3",{"name":"description","content":"Personal portfolio of Teng Teng, Lead of AI/ML Development & Research at SAP"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","link","5",{"rel":"icon","href":"/favicon.ico"}],["$","meta","6",{"name":"next-size-adjust"}]]
1:null
